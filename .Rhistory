N0 <- 120
M0 <- 101
sigma_set <- c(0.1, 0.5)
rout <- 400
H1 <- 0.8
H2 <- 0.5
xout <- seq(0, 1, length.out = M0)
delta_grid <- seq(1/sqrt(M0), 0.4, length.out = 15)
delta_learn <- (1 / sqrt(M0))
# Parameter settings for online set
M0_new_dense <- 201
M0_obs <- 101
xout_dense <- seq(0, 1, length.out = M0_new_dense)
xout_obs <- seq(0, 1, length.out = M0_obs)
# All configurations in loop
param_cart <- expand.grid(alpha = alpha_set,
sigma = sigma_set)
result_folder <- here('result_smooth', 'prod')
if(!dir.exists(result_folder)) {
dir.create(result_folder)
}
param_dir <- sapply(seq_len(nrow(param_cart)), function(k) {
paste0(result_folder,
"/alpha", round(param_cart[k, "alpha"], 2),
"_sigma", param_cart[k, "sigma"])
})
param_dir
param_cart
for(dir in param_dir) {
if(!dir.exists(dir)) {
dir.create(dir)
}
}
set.seed(1234)
seeds <- sample.int(10000, size = rout)
seeds_learn <- sample.int(10000, size = nrow(param_cart))
learn_simu <- function(k) {
alpha <- param_cart[k, "alpha"]
sigma <- param_cart[k, "sigma"]
set.seed(seeds_learn[k])
# Generate learning set
Y_sum_learn <- purrr::map(seq_len(N0),
~fbm_sheet(
t_n = M0,
e_n = M0,
alpha = alpha,
H1 = H1,
H2 = H2,
type = "prod",
sigma = sigma
)
) |>
purrr::map(~list(t = xout,
X = .x))
# Estimate & identify angle from the learning set
g_hat_sum <- estimate_angle(X_list = Y_sum_learn,
xout = xout,
delta = delta_learn)
alpha_hat_sum <- identify_angle(angles = g_hat_sum,
X_list = Y_sum_learn,
dout = delta_grid,
xout = xout,
sigma = g_hat_sum$sigma_hat)
# Perform correction for estimated angle
alpha_hat_sum_adj <- angle_correct(X_list = Y_sum_learn,
xout = xout,
g_hat = g_hat_sum$g_hat,
alpha_hat = alpha_hat_sum$alpha,
delta = delta_learn,
Hmax = alpha_hat_sum$H_max,
Hmin = g_hat_sum$H_min,
sigma = g_hat_sum$sigma_hat)
# Build anisotropic basis vectors
u1 <- unname(c(cos(alpha_hat_sum_adj$alpha), sin(alpha_hat_sum_adj$alpha)))
u2 <- unname(c(-sin(alpha_hat_sum_adj$alpha), cos(alpha_hat_sum_adj$alpha)))
H_ani <- c(alpha_hat_sum$H_max, g_hat_sum$H_min)
# Compute rotation matrix
R_alpha <- matrix(c(cos(alpha_hat_sum_adj$alpha_adj),
-sin(alpha_hat_sum_adj$alpha_adj),
sin(alpha_hat_sum_adj$alpha_adj),
cos(alpha_hat_sum_adj$alpha_adj)
), nrow = 2, ncol = 2)
# Comparison - Estimate H and L along the canonical basis
e1 <- c(1, 0)
e2 <- c(0, 1)
H_iso <- H_sheets_dir(X_list = Y_sum_learn,
tout = expand.grid(t1 = xout, t2 = xout),
delta = delta_learn,
base_list = list(e1, e2),
sigma = g_hat_sum$sigma_hat)
list('R_alpha' = R_alpha,
'H_ani' = H_ani,
'H_iso' = H_iso,
'alpha' = alpha,
'sigma' = sigma)
}
nrow(param_cart)
# Execute learning set - note! mclapply only works on Mac / Linux: only
# windows, it will run sequentially
learn_list <- mclapply(X = 1:nrow(param_cart),
FUN = function(x) learn_simu(k = x),
mc.cores = nrow(param_cart))
learn_list
result_list <- readRDS("~/direg/result_smooth/sum/result_list.rds")
result_list
library(dplyr)
library(tidyr)
library(stringr)
library(tibble)
library(tikzDevice)
df_smooth <- as_tibble(result_list)
df_smooth
rel_plot <- df_smooth |>
ggplot(aes(x = as.factor(sigma), y = risk_rel^2, fill = as.factor(alpha))) +
geom_boxplot() +
geom_hline(yintercept = 1, color = "red") +
xlab("$\\sigma$") +
ylab(paste0("$\\mathcal{R}_{rel}$")) +
theme_minimal() +
scale_fill_grey(start = 0.3, end = 1) +
ggtitle('Change-of-basis comparison') +
theme(plot.title = element_text(hjust = 0.5)) +
guides(fill=guide_legend(title= "$\\alpha$"))
library(ggplot2)
rel_plot <- df_smooth |>
ggplot(aes(x = as.factor(sigma), y = risk_rel^2, fill = as.factor(alpha))) +
geom_boxplot() +
geom_hline(yintercept = 1, color = "red") +
xlab("$\\sigma$") +
ylab(paste0("$\\mathcal{R}_{rel}$")) +
theme_minimal() +
scale_fill_grey(start = 0.3, end = 1) +
ggtitle('Change-of-basis comparison') +
theme(plot.title = element_text(hjust = 0.5)) +
guides(fill=guide_legend(title= "$\\alpha$"))
rel_plot
result_folder
result_folder <- here('result_smooth', 'sum')
library(here)
result_folder <- here('result_smooth', 'sum')
result_folder
result_folder
paste0(result_folder, "/plots", "/risk_rel.tex")
tikz(file = paste0(result_folder, "/plots", "/risk_rel.tex"),
height = 5,
width = 5,
standAlone = TRUE)
plot(rel_plot)
dev.off()
library(direg)
library(foreach)
library(parallel)
library(doSNOW)
library(ggplot2)
library(here)
library(tictoc)
alpha_set <- c(pi/30, pi/6, pi/4, pi/3, pi/2 - pi/30)
N0 <- 120
M0 <- 101
sigma_set <- c(0.1, 0.5)
rout <- 400
H1 <- 0.8
H2 <- 0.5
xout <- seq(0, 1, length.out = M0)
delta_grid <- seq(1/sqrt(M0), 0.4, length.out = 15)
delta_learn <- (1 / sqrt(M0))
# Parameter settings for online set
M0_new_dense <- 201
M0_obs <- 101
xout_dense <- seq(0, 1, length.out = M0_new_dense)
xout_obs <- seq(0, 1, length.out = M0_obs)
# All configurations in loop
param_cart <- expand.grid(alpha = alpha_set,
sigma = sigma_set)
result_folder <- here('result_smooth', 'prod')
if(!dir.exists(result_folder)) {
dir.create(result_folder)
}
param_dir <- sapply(seq_len(nrow(param_cart)), function(k) {
paste0(result_folder,
"/alpha", round(param_cart[k, "alpha"], 2),
"_sigma", param_cart[k, "sigma"])
})
param_dir
for(dir in param_dir) {
if(!dir.exists(dir)) {
dir.create(dir)
}
}
set.seed(1234)
seeds <- sample.int(10000, size = rout)
seeds_learn <- sample.int(10000, size = nrow(param_cart))
learn_simu <- function(k) {
alpha <- param_cart[k, "alpha"]
sigma <- param_cart[k, "sigma"]
set.seed(seeds_learn[k])
# Generate learning set
Y_sum_learn <- purrr::map(seq_len(N0),
~fbm_sheet(
t_n = M0,
e_n = M0,
alpha = alpha,
H1 = H1,
H2 = H2,
type = "prod",
sigma = sigma
)
) |>
purrr::map(~list(t = xout,
X = .x))
# Estimate & identify angle from the learning set
g_hat_sum <- estimate_angle(X_list = Y_sum_learn,
xout = xout,
delta = delta_learn)
alpha_hat_sum <- identify_angle(angles = g_hat_sum,
X_list = Y_sum_learn,
dout = delta_grid,
xout = xout,
sigma = g_hat_sum$sigma_hat)
# Perform correction for estimated angle
alpha_hat_sum_adj <- angle_correct(X_list = Y_sum_learn,
xout = xout,
g_hat = g_hat_sum$g_hat,
alpha_hat = alpha_hat_sum$alpha,
delta = delta_learn,
Hmax = alpha_hat_sum$H_max,
Hmin = g_hat_sum$H_min,
sigma = g_hat_sum$sigma_hat)
# Build anisotropic basis vectors
u1 <- unname(c(cos(alpha_hat_sum_adj$alpha), sin(alpha_hat_sum_adj$alpha)))
u2 <- unname(c(-sin(alpha_hat_sum_adj$alpha), cos(alpha_hat_sum_adj$alpha)))
H_ani <- c(alpha_hat_sum$H_max, g_hat_sum$H_min)
# Compute rotation matrix
R_alpha <- matrix(c(cos(alpha_hat_sum_adj$alpha_adj),
-sin(alpha_hat_sum_adj$alpha_adj),
sin(alpha_hat_sum_adj$alpha_adj),
cos(alpha_hat_sum_adj$alpha_adj)
), nrow = 2, ncol = 2)
# Comparison - Estimate H and L along the canonical basis
e1 <- c(1, 0)
e2 <- c(0, 1)
H_iso <- H_sheets_dir(X_list = Y_sum_learn,
tout = expand.grid(t1 = xout, t2 = xout),
delta = delta_learn,
base_list = list(e1, e2),
sigma = g_hat_sum$sigma_hat)
list('R_alpha' = R_alpha,
'H_ani' = H_ani,
'H_iso' = H_iso,
'alpha' = alpha,
'sigma' = sigma)
}
learn_list <- mclapply(X = 1:nrow(param_cart),
FUN = function(x) learn_simu(k = x),
mc.cores = nrow(param_cart))
result_folder
paste0(result_folder, "/learn_list.rds")
saveRDS(learn_list,
file = paste0(result_folder, "/learn_list.rds"))
n_cores <- 40
# Create cluster nodes
cl <- makeCluster(spec = n_cores)
# Register cluster
registerDoSNOW(cl)
(sqrt(0.5) / sqrt(0.2))^2
0.5 / 0.2
param_dir
tic()
result_list <- foreach(k = 1:length(learn_list),
.combine = 'rbind') %do% {
alpha <- learn_list[[k]]$alpha
sigma <- learn_list[[k]]$sigma
R_alpha <- learn_list[[k]]$R_alpha
H_ani <- learn_list[[k]]$H_ani
H_iso <- learn_list[[k]]$H_iso
foreach(i = 1:rout,
.packages = c("direg", "parallel", "snow"),
.combine = 'rbind')  %dopar% {
set.seed(seeds[i])
# Generate true curve on a dense grid, first without noise, then
# manually add noise to the discretised ones
# First without noise
Y_new_true <- purrr::map(seq_len(1),
~fbm_sheet(
t_n = M0_new_dense,
e_n = M0_new_dense,
alpha = alpha,
H1 = H1,
H2 = H2,
type = "prod",
sigma = 0)
) |>
purrr::map(~list(t = xout_dense,
X = .x))
# Discretise the true surface with interpolation
tobs <- expand.grid(t1 = xout_obs,
t2 = xout_obs)
Y_new_obs <- purrr::map(Y_new_true,
~pracma::interp2(x = .x$t,
y = .x$t,
Z = .x$X,
xp = tobs[, 2],
yp = tobs[, 1],
method = "nearest") |>
matrix(nrow = M0_obs,
ncol = M0_obs)
)
# Now add some noise
Y_new_noisy <- purrr::map(Y_new_obs,
~list(t = xout_obs,
X = .x + rnorm(n = M0_obs^2,
sd = sigma)
)
)
# Now construct the smoothing grid - we need to transform it onto the
# anisotropic basis!
tout <- expand.grid(t1 = xout_dense,
t2 = xout_dense)
# Rotation on the smoothing grid
tout_rot <- t(apply(tout, 1,
function(x) crossprod(t(R_alpha), x)))
# Perform rotation on the observed grid
tobs_rot <- t(apply(tobs, 1,
function(x) crossprod(t(R_alpha), x)))
# Compute optimal smoothing bandwidth
h_star <- bw_smooth(H1 = H_ani[1],
H2 = H_ani[2],
M0 = M0_obs^2)
# Compute smoothing bandwidth along canonical basis (w/o directional reg)
h_star_iso <- bw_smooth(H1 = H_iso[1],
H2 = H_iso[2],
M0 = M0_obs^2)
# Smooth surfaces
Y_smoothed <- ksmooth_bi(Y_list = Y_new_noisy,
bw_vec = h_star,
tobs = tobs_rot,
tout = tout_rot)
Y_smoothed_iso <- ksmooth_bi(Y_list = Y_new_noisy,
bw_vec = h_star_iso,
tobs = tobs,
tout = tout)
# Compute the risk
risk_ani <- purrr::map2_dbl(Y_new_true, Y_smoothed,
~mean((.x$X - .y)^2))
risk_iso <- purrr::map2_dbl(Y_new_true, Y_smoothed_iso,
~mean((.x$X - .y)^2))
risk_rel <- risk_ani / risk_iso
result <- c('risk_ani' = risk_ani,
'risk_iso' = risk_iso,
'risk_rel' = risk_rel,
'h1_ani' = h_star[1],
'h2_ani' = h_star[2],
'h1_iso' = h_star_iso[1],
'h2_iso' = h_star_iso[2],
'H1_ani' = H_ani[1],
'H2_ani' = H_ani[2],
'H1_iso' = H_iso[1],
'H2_iso' = H_iso[2],
'alpha' = round(alpha, 2),
'sigma' = sigma)
saveRDS(result,
file = paste0(param_dir[k], "/result_", i, ".rds"))
result
}
}
library(direg)
library(foreach)
library(parallel)
library(doSNOW)
library(ggplot2)
library(here)
library(tictoc)
alpha_set <- c(pi/30, pi/6, pi/4, pi/3, pi/2 - pi/30)
N0 <- 120
M0 <- 101
sigma_set <- c(0.1, 0.5)
rout <- 400
H1 <- 0.8
H2 <- 0.5
xout <- seq(0, 1, length.out = M0)
delta_grid <- seq(1/sqrt(M0), 0.4, length.out = 15)
delta_learn <- (1 / sqrt(M0))
# Parameter settings for online set
M0_new_dense <- 201
M0_obs <- 101
xout_dense <- seq(0, 1, length.out = M0_new_dense)
xout_obs <- seq(0, 1, length.out = M0_obs)
# All configurations in loop
param_cart <- expand.grid(alpha = alpha_set,
sigma = sigma_set)
result_folder <- here('result_smooth', 'prod')
if(!dir.exists(result_folder)) {
dir.create(result_folder)
}
param_dir <- sapply(seq_len(nrow(param_cart)), function(k) {
paste0(result_folder,
"/alpha", round(param_cart[k, "alpha"], 2),
"_sigma", param_cart[k, "sigma"])
})
for(dir in param_dir) {
if(!dir.exists(dir)) {
dir.create(dir)
}
}
set.seed(1234)
seeds <- sample.int(10000, size = rout)
seeds_learn <- sample.int(10000, size = nrow(param_cart))
learn_list <- readRDS("~/direg/result_smooth/prod/learn_list.rds")
n_cores <- 40
# Create cluster nodes
cl <- makeCluster(spec = n_cores)
# Register cluster
registerDoSNOW(cl)
learn_list
tic()
result_list <- foreach(k = 1:length(learn_list),
.combine = 'rbind') %do% {
alpha <- learn_list[[k]]$alpha
sigma <- learn_list[[k]]$sigma
R_alpha <- learn_list[[k]]$R_alpha
H_ani <- learn_list[[k]]$H_ani
H_iso <- learn_list[[k]]$H_iso
foreach(i = 1:rout,
.packages = c("direg", "parallel", "snow"),
.combine = 'rbind')  %dopar% {
set.seed(seeds[i])
# Generate true curve on a dense grid, first without noise, then
# manually add noise to the discretised ones
# First without noise
Y_new_true <- purrr::map(seq_len(1),
~fbm_sheet(
t_n = M0_new_dense,
e_n = M0_new_dense,
alpha = alpha,
H1 = H1,
H2 = H2,
type = "prod",
sigma = 0)
) |>
purrr::map(~list(t = xout_dense,
X = .x))
# Discretise the true surface with interpolation
tobs <- expand.grid(t1 = xout_obs,
t2 = xout_obs)
Y_new_obs <- purrr::map(Y_new_true,
~pracma::interp2(x = .x$t,
y = .x$t,
Z = .x$X,
xp = tobs[, 2],
yp = tobs[, 1],
method = "nearest") |>
matrix(nrow = M0_obs,
ncol = M0_obs)
)
# Now add some noise
Y_new_noisy <- purrr::map(Y_new_obs,
~list(t = xout_obs,
X = .x + rnorm(n = M0_obs^2,
sd = sigma)
)
)
# Now construct the smoothing grid - we need to transform it onto the
# anisotropic basis!
tout <- expand.grid(t1 = xout_dense,
t2 = xout_dense)
# Rotation on the smoothing grid
tout_rot <- t(apply(tout, 1,
function(x) crossprod(t(R_alpha), x)))
# Perform rotation on the observed grid
tobs_rot <- t(apply(tobs, 1,
function(x) crossprod(t(R_alpha), x)))
# Compute optimal smoothing bandwidth
h_star <- bw_smooth(H1 = H_ani[1],
H2 = H_ani[2],
M0 = M0_obs^2)
# Compute smoothing bandwidth along canonical basis (w/o directional reg)
h_star_iso <- bw_smooth(H1 = H_iso[1],
H2 = H_iso[2],
M0 = M0_obs^2)
# Smooth surfaces
Y_smoothed <- ksmooth_bi(Y_list = Y_new_noisy,
bw_vec = h_star,
tobs = tobs_rot,
tout = tout_rot)
Y_smoothed_iso <- ksmooth_bi(Y_list = Y_new_noisy,
bw_vec = h_star_iso,
tobs = tobs,
tout = tout)
# Compute the risk
risk_ani <- purrr::map2_dbl(Y_new_true, Y_smoothed,
~mean((.x$X - .y)^2))
risk_iso <- purrr::map2_dbl(Y_new_true, Y_smoothed_iso,
~mean((.x$X - .y)^2))
risk_rel <- risk_ani / risk_iso
result <- c('risk_ani' = risk_ani,
'risk_iso' = risk_iso,
'risk_rel' = risk_rel,
'h1_ani' = h_star[1],
'h2_ani' = h_star[2],
'h1_iso' = h_star_iso[1],
'h2_iso' = h_star_iso[2],
'H1_ani' = H_ani[1],
'H2_ani' = H_ani[2],
'H1_iso' = H_iso[1],
'H2_iso' = H_iso[2],
'alpha' = round(alpha, 2),
'sigma' = sigma)
saveRDS(result,
file = paste0(param_dir[k], "/result_", i, ".rds"))
result
}
}
